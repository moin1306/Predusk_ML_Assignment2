# -*- coding: utf-8 -*-
"""Predusk_ML_ass2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TShBd1xDPuDbxuWqTLAYmDjU76jRb7PV
"""

!pip install transformers

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

prompt = "Once upon a time"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

#   Generate text with temperature = 0.7
output_07 = model.generate(
    input_ids,
    max_length=input_ids.shape[1] + 50,
    do_sample=True,
    top_k=50,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)


text_temp_07 = tokenizer.decode(output_07[0], skip_special_tokens=True)
print("Temperature = 0.7 \n Output:", text_temp_07)

with open("output_temp_07.txt", "w") as f:
    f.write(text_temp_07)

#  Generate text with temperature = 1.0
output_10 = model.generate(
    input_ids,
    max_length=input_ids.shape[1] + 50,
    do_sample=True,
    top_k=50,
    temperature=1.0,
    pad_token_id=tokenizer.eos_token_id
)


text_temp_10 = tokenizer.decode(output_10[0], skip_special_tokens=True)
print(" Temperature = 1.0 \n  Output:", text_temp_10)

with open("output_temp_10.txt", "w") as f:
    f.write(text_temp_10)

def calculate_perplexity(text, model, tokenizer):
    enc = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        out = model(**enc, labels=enc["input_ids"])
        loss = out.loss
    return torch.exp(loss).item()

ppl_07 = calculate_perplexity(text_temp_07, model, tokenizer)
ppl_10 = calculate_perplexity(text_temp_10, model, tokenizer)


print("Perplexity of text at 0.7 temperature:", ppl_07)
print("Perplexity of text at 1.0 temperature:", ppl_10)